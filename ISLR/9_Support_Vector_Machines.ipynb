{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Maximal Margin Classifier（マージン最大化分類）\n",
    "\n",
    "### 1. 超平面って何？\n",
    "\n",
    "p次元のデータに対して，(p-1)次元の平面で切り分ける。この(p-1)次元の平面をpの値によらず「超平面」という\n",
    "\n",
    "p次元のデータを切り分ける平面の式はこのようになる\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p = 0 \\tag{9.2}\n",
    "$$\n",
    "実際には，\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p > 0 \\tag{9.3}\n",
    "$$\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p < 0 \\tag{9.4}\n",
    "$$\n",
    "のどちらを満たすかで，超平面のどちら側であるのかを定める\n",
    "\n",
    "個人的には，超平面については，ML_deepさんの(SVM、ニューラルネットなどに共通する分類問題における考え方)[ https://www.hellocybernetics.tech/entry/2017/03/08/053000 ]のページにおける超平面の考え方がわかりやすいと思う\n",
    "\n",
    "### 2. 分離超平面を用いた分類\n",
    "\n",
    "$y_i = 1$の時（9.3）式，$y_i = -1$の時（9.4）式を適応させる。これらは，次の式（9.8）と同義である。\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip}) > 0 \\tag{9.8}\n",
    "$$\n",
    "ここで，$f(x^*) = \\beta_0 + \\beta_1 X_1^* + \\beta_2 X_2^* + \\dots + \\beta_p X_p^*$として，$f(x^*)$が0よりも離れている程，よく分類できており，0に近い程，曖昧な分類となる\n",
    "\n",
    "### 3. マージン最大化分類\n",
    "\n",
    "別名「最適分離超平面」，訓練データから分離超平面を最も離れさせる方法である。平面から最も距離が小さいデータをマージンと呼び，このマージンとの距離を最大化させる。実際には$f(x^*)$が最小となるマージンの$f(x^*)$を最大化させる。この方法はよく上手くいくが，pが大きすぎると過学習してしまう。\n",
    "\n",
    "マージンと同じ距離のデータを「サポートベクトル」といい，マージン最大化平面を決定する「サポート」を行うデータである。\n",
    "\n",
    "### 4. マージン最大化分類の構築\n",
    "\n",
    "マージンMの最大化は，最適化問題として以下から導くことができる。\n",
    "$$\n",
    "\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, M}{maximize} M \\tag{9.9}\n",
    "$$\n",
    "$$\n",
    "\\sum_{j=1}^p \\beta_j^2 = 1 \\tag{9.10}\n",
    "$$\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip}) \\geq M \\ \\forall \\ i = 1, \\dots, n \\tag{9.11}\n",
    "$$\n",
    "式（9.10）によって，$y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip})$が超平面からの距離となる。\n",
    "\n",
    "### 5. 分けられない場合\n",
    "\n",
    "本来，分離可能な超平面が存在する方が稀である。そこで，誤分類を認めた超平面を作成するソフトマージンを使用する。このケースがサポートベクトル分類である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Classifiers（サポートベクトル分類）\n",
    "\n",
    "### 1. サポートベクトル分類の概要\n",
    "\n",
    "個々の観測に対する堅牢性が向上し、ほとんどの訓練観測の分類が改善される。\n",
    "\n",
    "### 2. サポートベクトル分類の詳細\n",
    "\n",
    "誤分類を多少許すので，サポートベクトル分類の最適化問題は次のようになる。\n",
    "$$\n",
    "\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, \\epsilon_1, \\dots, \\epsilon_n, M}{maximize} M \\tag{9.12}\n",
    "$$\n",
    "$$\n",
    "\\sum_{j=1}^p \\beta_j^2 = 1 \\tag{9.13}\n",
    "$$\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip}) \\geq M(1 - \\epsilon_i) \\ \\forall \\ i = 1, \\dots, n \\tag{9.14}\n",
    "$$\n",
    "$$\n",
    "\\epsilon_i \\geq 0, \\sum_{i=1}^n \\epsilon_i \\leq C \\tag{9.15}\n",
    "$$\n",
    "ここで，Cは0以上のチューニングパラメータである。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
