{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Maximal Margin Classifier（マージン最大化分類）\n",
    "\n",
    "### 1. 超平面って何？\n",
    "\n",
    "p次元のデータに対して，(p-1)次元の平面で切り分ける。この(p-1)次元の平面をpの値によらず「超平面」という\n",
    "\n",
    "p次元のデータを切り分ける平面の式はこのようになる\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p = 0 \\tag{9.2}\n",
    "$$\n",
    "実際には，\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p > 0 \\tag{9.3}\n",
    "$$\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p < 0 \\tag{9.4}\n",
    "$$\n",
    "のどちらを満たすかで，超平面のどちら側であるのかを定める\n",
    "\n",
    "個人的には，超平面については，ML_deepさんの(SVM、ニューラルネットなどに共通する分類問題における考え方)[ https://www.hellocybernetics.tech/entry/2017/03/08/053000 ]のページにおける超平面の考え方がわかりやすいと思う\n",
    "\n",
    "### 2. 分離超平面を用いた分類\n",
    "\n",
    "$y_i = 1$の時（9.3）式，$y_i = -1$の時（9.4）式を適応させる。これらは，次の式（9.8）と同義である。\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip}) > 0 \\tag{9.8}\n",
    "$$\n",
    "ここで，$f(x^*) = \\beta_0 + \\beta_1 X_1^* + \\beta_2 X_2^* + \\dots + \\beta_p X_p^*$として，$f(x^*)$が0よりも離れている程，よく分類できており，0に近い程，曖昧な分類となる\n",
    "\n",
    "### 3. マージン最大化分類\n",
    "\n",
    "別名「最適分離超平面」，訓練データから分離超平面を最も離れさせる方法である。平面から最も距離が小さいデータをマージンと呼び，このマージンとの距離を最大化させる。実際には$f(x^*)$が最小となるマージンの$f(x^*)$を最大化させる。この方法はよく上手くいくが，pが大きすぎると過学習してしまう。\n",
    "\n",
    "マージンと同じ距離のデータを「サポートベクトル」といい，マージン最大化平面を決定する「サポート」を行うデータである。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
